## TODO

Finish this today/tmrw
-----------------------------------------------------
<<<<<<< HEAD
- [ ] make deployment schemas work
- [ ] make datasets be stored as pandas dataframes. preprocessing can be done on a training-run level


- [ ] Dataset versioning
    - each dataset is generated in a jupyter notebook and, as final step, optionally uploaded to mlflow
    - these notebook should have no dependencies (copy over utils to previous cells)
    - Each dataset has a special mlflow experiment, where a run is a generation of a particular version of the dataset.
    - The run uploads the generated dataset + notebook as artifacts in the end
        - (not hash but actual notebook, so that this works without git pushes. prolly implement by literally reading current file)
        - also, in the notebook,  include docs on how to get the raw data for running the notebook i
    - 2 kinds of transforms:
        - simple augmentations like adding noise, done at training time
        - theory-based augmentations to expand dataset at time of versioned dataset generation
=======
- [ ] fix config and main, simplify if can
>>>>>>> c9e67aacb6a1df4ea0423ab5ba270e44b5dbf826

- [ ] rebuild images without datasets and test all 3 modes

- [ ] quick model deployment
    - [ ] fix log_model to work properly
    - [ ] add input/output schemas and descriptions. Model output as dict.
    - [ ] serve app with its own dockerfile (just my own rest api). Use env variable of model to get model.
    - [ ] basic demo app which connects with server through docker compose

- [ ] extra logs
    - [ ] logs instead of prints
    - [ ] saving to file and uploading artifact
    - [ ] resource usage logs

------------------------------------------------------
- [ ] debug, general improvements, improve documentation (both readme and standardized docstrings + autogenerated docs)

- [ ] improve evaluation, incl neater (in terms of mlflowui) per-class organisation, recall, precission and visualizations (confusion mat)

- [ ] improve training (LR scheduling and good optimizer)
    - [ ] pos weights implemented correctly?

- [ ] implement and train fcg transformer

- [ ] visualize attention

------------------------------------------------------

- [ ] model improvement

- [ ] cloud deployment scripts + hyperparam search algos



## Datasets
- 2 kinds of transforms:
    - simple augmentations like adding noise, done at training time
    - theory-based augmentations to expand dataset at time of versioned dataset generation

### FTIR
- to use the FTIR dataset used in the FCG-Former paper:
    - download `dataset.zip` from https://huggingface.co/datasets/lycaoduong/FTIR/tree/main 
    - unzip it in `data/ftir`


## Docker
- `./Dockerfile.base` only installs dependencies
- `./Dockerfile.train` also copies over files. Used to build an image for a training job deployable to cloud.
- `.devcontainer/Dockerfile` also adds a non-root user and installs dev tools
- For the dev-container, MLFlow authentication environment variables are loaded from local `.env` or your shell
- For deploying jobs, will need to specify in the deployment command, along with hyparams

## Building docker images
- `make` build all 3 docker images. 
- can use args `build-base/build-train/build-dev` to only build some
- `make clean` removes the images