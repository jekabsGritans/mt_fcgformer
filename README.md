## TODO

Finish this today/tmrw
-----------------------------------------------------
- [ ] make deployment schemas work
- [ ] make datasets be stored as pandas dataframes. preprocessing can be done on a training-run level


- [ ] Dataset versioning
    - each dataset is generated in a jupyter notebook and, as final step, optionally uploaded to mlflow
    - these notebook should have no dependencies (copy over utils to previous cells)
    - Each dataset has a special mlflow experiment, where a run is a generation of a particular version of the dataset.
    - The run uploads the generated dataset + notebook as artifacts in the end
        - (not hash but actual notebook, so that this works without git pushes. prolly implement by literally reading current file)
        - also, in the notebook,  include docs on how to get the raw data for running the notebook i
    - 2 kinds of transforms:
        - simple augmentations like adding noise, done at training time
        - theory-based augmentations to expand dataset at time of versioned dataset generation

- [ ] SYNC CLEARLY WITH TEAM FOR DATASET CREATION PATTERNS

- [ ] single class for loading those datasets for train/test
    - probably means that they should be stored in a standardized format like csv

- [ ] rebuild images without datasets

- [ ] figure out model deployment
    - probably through log_model in mlflow but then figure out errors and dict outputs
    - add input/output schemas and descriptions

- [ ] log git hash for training code of a run

- [ ] logs instead of prints, and also resource usage logs, upload as artifacts

------------------------------------------------------
- [ ] improve documentation (both readme and standardized docstrings + autogenerated docs)

- [ ] improve evaluation, incl neater (in terms of mlflowui) per-class organisation, recall, precission and visualizations (confusion mat)

- [ ] improve training (LR scheduling and good optimizer)
    - [ ] pos weights implemented correctly?

- [ ] implement and train fcg transformer

- [ ] visualize attention

------------------------------------------------------

- [ ] model improvement

- [ ] cloud deployment scripts + hyperparam search algos



## Datasets
### FTIR
- to use the FTIR dataset used in the FCG-Former paper:
    - download `dataset.zip` from https://huggingface.co/datasets/lycaoduong/FTIR/tree/main 
    - unzip it in `data/ftir`


## Docker
- `./Dockerfile.base` only installs dependencies
- `./Dockerfile.train` also copies over files. Used to build an image for a training job deployable to cloud.
- `.devcontainer/Dockerfile` also adds a non-root user and installs dev tools
- For the dev-container, MLFlow authentication environment variables are loaded from local `.env` or your shell
- For deploying jobs, will need to specify in the deployment command, along with hyparams

## Building docker images
- `make` build all 3 docker images. 
- can use args `build-base/build-train/build-dev` to only build some
- `make clean` removes the images