{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1466c028",
   "metadata": {},
   "source": [
    "%%markdown\n",
    "# Dataset Generation Template\n",
    "\n",
    "This notebook provides a template for generating and uploading new datasets to MLflow, after you have the raw scraped data.\n",
    "\n",
    "## Data Format\n",
    "\n",
    "Your data needs to be organized in a dictionary with three required keys:\n",
    "- `inputs`: Feature matrix as NumPy array\n",
    "- `target`: Binary target matrix as NumPy array\n",
    "- `target_names`: List of strings naming each target feature\n",
    "\n",
    "### Shape Requirements\n",
    "\n",
    "| Component | Type | Shape | Description | Example |\n",
    "|-----------|------|-------|-------------|----------|\n",
    "| `inputs` | `np.ndarray` | `(n_samples, n_features)` | Each row is one sample, each column a feature | `(1000, 10)` for 1000 samples with 10 features |\n",
    "| `target` | `np.ndarray` | `(n_samples, n_targets)` | Binary matrix where each column represents one target | `(1000, 3)` for 1000 samples with 3 possible targets |\n",
    "| `target_names` | `list[str]` | `(n_targets,)` | Names for each target column | `[\"cat\", \"dog\", \"bird\"]` for 3 targets |\n",
    "\n",
    "## Example\n",
    "\n",
    "```python\n",
    "data = {\n",
    "    \"inputs\": np.array([\n",
    "        [0.1, 0.2, 0.3],  # Sample 1 with 3 features\n",
    "        [0.4, 0.5, 0.6],  # Sample 2 with 3 features\n",
    "        # ... more samples\n",
    "    ]),\n",
    "    \"target\": np.array([\n",
    "        [1, 0],  # Sample 1: positive for target 1, negative for target 2\n",
    "        [0, 1],  # Sample 2: negative for target 1, positive for target 2\n",
    "        # ... more samples\n",
    "    ]),\n",
    "    \"target_names\": [\"group_A\", \"group_B\"]  # Names for the two target columns\n",
    "}\n",
    "```\n",
    "\n",
    "## Uploading the dataset\n",
    "Once your data is prepared, use `upload_dataset()` to save it to MLflow.\n",
    "This will verify that the data is formatted correctly and then upload it to the server.\n",
    "\n",
    "```python\n",
    "upload_dataset(\n",
    "    data=data,\n",
    "    dataset_name=\"ftir_no_bonding_effects\",  # Broader dataest for which we can have multiple versions\n",
    "    version_name=\"initial_data\",  # Description of this version\n",
    "    description=\"FTIR dataset downloaded from the FCGFormer paper without any modifications\"  # Optional details\n",
    ")\n",
    "```\n",
    "\n",
    "## Accessing the Dataset\n",
    "\n",
    "After upload, the dataset will be available in MLflow for model training with:\n",
    "- NumPy arrays saved as `.npy` files\n",
    "- Target names and counts (number of positive examples) in text files\n",
    "- The code of this notebook saved for reproducability (so you don't have to upload it anywhere)\n",
    "\n",
    "You can view your dataset in MLflow by opening the link printed after calling `upload_dataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d458a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell defines upload_dataset. You can ignore it.\n",
    "\n",
    "# Install required packages for `upload_dataset()`\n",
    "%pip install numpy mlflow ipynbname requests\n",
    "\n",
    "import os\n",
    "import urllib.parse\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from typing import Dict, Any\n",
    "import jupyter_client\n",
    "try:\n",
    "    from notebook import notebookapp\n",
    "except ImportError:\n",
    "    from jupyter_server import serverapp as notebookapp\n",
    "\n",
    "# MLFlow creds\n",
    "MLFLOW_DOMAIN = \"mlflow.gritans.lv\"\n",
    "MLFLOW_USERNAME = \"data_user\"\n",
    "MLFLOW_PASSWORD = \"ais7Rah2foo0gee9\"\n",
    "MLFLOW_TRACKING_URI = f\"https://{MLFLOW_DOMAIN}\"\n",
    "\n",
    "parsed_uri = urllib.parse.urlparse(MLFLOW_TRACKING_URI)\n",
    "auth_uri = parsed_uri._replace(\n",
    "    netloc=f\"{urllib.parse.quote(MLFLOW_USERNAME)}:{urllib.parse.quote(MLFLOW_PASSWORD)}@{parsed_uri.netloc}\"\n",
    ").geturl()\n",
    "\n",
    "mlflow.set_tracking_uri(auth_uri)\n",
    "\n",
    "\n",
    "def upload_dataset(\n",
    "    data: Dict[str, Any],\n",
    "    dataset_name: str,\n",
    "    version_name: str,\n",
    "    description: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (Dict[str, Any]): Dictionary containing the dataset with keys:\n",
    "            - \"inputs\": NumPy array of shape (num_samples, num_input_features)\n",
    "            - \"target\": NumPy array of shape (num_samples, num_output_features)\n",
    "            - \"target_names\": List of target feature names, in the same order as the target array.\n",
    "        dataset_name (str): Name of the dataset.\n",
    "        version_name (str): A descriptive version name for the dataset. Doesn't need to be unique, just for reference.\n",
    "        description (str): An (optional) description of this dataset version.\n",
    "    \"\"\"\n",
    "    # Check dictionary\n",
    "    expected_keys = {\"inputs\", \"target\", \"target_names\"}\n",
    "    assert set(data.keys()) == expected_keys, (\n",
    "        f\"Invalid dataset format. Keys should be {expected_keys}.\"\n",
    "    )\n",
    "\n",
    "    # Check expected types\n",
    "    assert isinstance(data[\"inputs\"], np.ndarray), (\n",
    "        f\"Inputs must be a numpy.ndarray. Got {type(data['inputs'])}.\"\n",
    "    )\n",
    "    assert isinstance(data[\"target\"], np.ndarray), (\n",
    "        f\"Targets must be a numpy.ndarray. Got {type(data['target'])}.\"\n",
    "    )\n",
    "    assert isinstance(data[\"target_names\"], list), (\n",
    "        f\"target names must be a list. Got {type(data['target_names'])}.\"\n",
    "    )\n",
    "    assert all(isinstance(name, str) for name in data[\"target_names\"]), (\n",
    "        \"All target names must be strings.\"\n",
    "    )\n",
    "\n",
    "    # Check expected shapes\n",
    "    inputs: np.ndarray = data[\"inputs\"]\n",
    "    target: np.ndarray = data[\"target\"]\n",
    "    target_names = data[\"target_names\"]\n",
    "\n",
    "    assert inputs.ndim == 2, (\n",
    "        f\"Inputs must be a (num_samples, num_input_features) array. \"\n",
    "        f\"Got {inputs.ndim} dimensions.\"\n",
    "    )\n",
    "    assert target.ndim == 2, (\n",
    "        f\"Targets must be a (num_samples, num_output_features) array. \"\n",
    "        f\"Got {target.ndim} dimensions.\"\n",
    "    )\n",
    "\n",
    "    n_samples = inputs.shape[0]\n",
    "    assert n_samples > 0, (\n",
    "        f\"Inputs must have at least one sample. Got {n_samples} samples.\"\n",
    "    )\n",
    "    assert n_samples == target.shape[0], (\n",
    "        f\"Inputs and targets must have the same number of samples. \"\n",
    "        f\"Got {n_samples} inputs and {target.shape[0]} targets.\"\n",
    "    )\n",
    "\n",
    "    n_outputs = target.shape[1]\n",
    "    assert n_outputs > 0 and n_outputs == len(target_names), (\n",
    "        f\"Targets must have the same number of features as target names. \"\n",
    "        f\"Got {n_outputs} target features and {len(target_names)} target names.\"\n",
    "    )\n",
    "\n",
    "    # Compute number of positive samples per target\n",
    "    pos_counts = target.sum(axis=0)\n",
    "\n",
    "    mlflow.set_experiment(experiment_name=dataset_name)\n",
    "    with mlflow.start_run(run_name=version_name) as run:\n",
    "        local_dir = os.path.join(\"./runs\", run.info.run_id)\n",
    "        os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "        # Log the notebook generating this dataset\n",
    "  \n",
    "    try:\n",
    "        # primary: ipynbname often just works\n",
    "        import ipynbname\n",
    "        notebook_path = ipynbname.path()\n",
    "\n",
    "    except Exception:\n",
    "        # fallback: query the Jupyter serverâ€™s /api/sessions\n",
    "        # 1) get your kernel id\n",
    "        conn_file = jupyter_client.find_connection_file()\n",
    "        kernel_id = os.path.basename(conn_file).split('-', 1)[1].split('.')[0]\n",
    "\n",
    "        # 2) iterate over all running notebook servers\n",
    "        for srv in notebookapp.list_running_servers():\n",
    "            # build the URL for sessions\n",
    "            url = srv['url'].rstrip('/') + '/api/sessions'\n",
    "            token = srv.get('token', '')\n",
    "            params = {'token': token} if token else {}\n",
    "\n",
    "            try:\n",
    "                resp = requests.get(url, params=params)\n",
    "                resp.raise_for_status()\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            # 3) look for our kernel in their active sessions\n",
    "            for sess in resp.json():\n",
    "                if sess['kernel']['id'] == kernel_id:\n",
    "                    # 4) reconstruct the full path\n",
    "                    rel_path = sess['notebook']['path']       # e.g. \"subdir/MyNotebook.ipynb\"\n",
    "                    notebook_path = os.path.join(srv['notebook_dir'], rel_path)\n",
    "                    break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "        else:\n",
    "            raise RuntimeError(\"Could not locate the current notebook path\")\n",
    "\n",
    "        filename = os.path.basename(notebook_path)\n",
    "        mlflow.log_artifact(notebook_path, filename)\n",
    "\n",
    "        # Log the dataset\n",
    "        inputs_path = os.path.join(local_dir, \"inputs.npy\")\n",
    "        target_path = os.path.join(local_dir, \"target.npy\")\n",
    "        target_names_path = os.path.join(local_dir, \"target_names.txt\")\n",
    "        pos_counts_path = os.path.join(local_dir, \"pos_counts.txt\")\n",
    "\n",
    "        ## save locally\n",
    "        np.save(inputs_path, inputs)\n",
    "        np.save(target_path, target)\n",
    "\n",
    "        with open(target_names_path, \"w\") as f:\n",
    "            for name in target_names:\n",
    "                f.write(f\"{name}\\n\")\n",
    "\n",
    "        with open(pos_counts_path, \"w\") as f:\n",
    "            for i, count in enumerate(pos_counts):\n",
    "                f.write(f\"{target_names[i]}: {count}\\n\")\n",
    "\n",
    "        ## upload to mlflow\n",
    "        mlflow.log_artifact(inputs_path)\n",
    "        mlflow.log_artifact(target_path)\n",
    "        mlflow.log_artifact(target_names_path)\n",
    "        mlflow.log_artifact(pos_counts_path)\n",
    "\n",
    "        # Log parameters for browsing\n",
    "        mlflow.log_param(\"target_names\", target_names)\n",
    "        mlflow.log_param(\"input_features\", inputs.shape[1])\n",
    "        mlflow.log_param(\"output_features\", target.shape[1])\n",
    "        mlflow.log_param(\"num_samples\", n_samples)\n",
    "\n",
    "        pos_counts_dict = {name: count for name, count in zip(target_names, pos_counts)}\n",
    "        mlflow.log_param(\"pos_counts\", pos_counts_dict)\n",
    "\n",
    "        # Log description\n",
    "        if description:\n",
    "            mlflow.set_tag(\"description\", description)\n",
    "        \n",
    "        # Print MLflow experiment URL\n",
    "        experiment = mlflow.get_experiment_by_name(dataset_name)\n",
    "        if experiment:\n",
    "            print(\"\\nAccess your dataset:\")\n",
    "            print(f\"View dataset at: https://{MLFLOW_USERNAME}:{MLFLOW_PASSWORD}@{MLFLOW_DOMAIN}/#/experiments/{experiment.experiment_id}\")\n",
    "            print(f\"View this version at: https://{MLFLOW_USERNAME}:{MLFLOW_PASSWORD}@{MLFLOW_DOMAIN}/#/experiments/{experiment.experiment_id}/runs/{run.info.run_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa7a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template cell - replace with your data processing code\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load/prepare your data\n",
    "inputs = None  # TODO: Replace with your feature matrix, shape (n_samples, n_features)\n",
    "target = None  # TODO: Replace with your target matrix, shape (n_samples, n_classes)\n",
    "target_names = []  # TODO: Replace with your class names\n",
    "\n",
    "# Step 2: Package and upload\n",
    "data = {\n",
    "    \"inputs\": inputs,\n",
    "    \"target\": target,\n",
    "    \"target_names\": target_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4376590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_dataset(\n",
    "    data=data,\n",
    "    dataset_name = ...,\n",
    "    version_name = ..., \n",
    "    description = None # Optional\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
