{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1466c028",
   "metadata": {},
   "source": [
    "# Dataset Generation Template\n",
    "\n",
    "This notebook provides a template for generating and uploading new datasets to MLflow, after you have the raw scraped data.\n",
    "\n",
    "## Data Format\n",
    "\n",
    "Your data needs to be organized in a dictionary with three required keys:\n",
    "- `inputs`: Feature matrix as NumPy array\n",
    "- `target`: Binary target matrix as NumPy array\n",
    "- `target_names`: List of strings naming each target feature\n",
    "\n",
    "### Shape Requirements\n",
    "\n",
    "| Component | Type | Shape | Description | Example |\n",
    "|-----------|------|-------|-------------|----------|\n",
    "| `inputs` | `np.ndarray` | `(n_samples, n_features)` | Each row is one sample, each column a feature | `(1000, 10)` for 1000 samples with 10 features |\n",
    "| `target` | `np.ndarray` | `(n_samples, n_targets)` | Binary matrix where each column represents one target | `(1000, 3)` for 1000 samples with 3 possible targets |\n",
    "| `target_names` | `list[str]` | `(n_targets,)` | Names for each target column | `[\"cat\", \"dog\", \"bird\"]` for 3 targets |\n",
    "\n",
    "## Example\n",
    "\n",
    "```python\n",
    "data = {\n",
    "    \"inputs\": np.array([\n",
    "        [0.1, 0.2, 0.3],  # Sample 1 with 3 features\n",
    "        [0.4, 0.5, 0.6],  # Sample 2 with 3 features\n",
    "        # ... more samples\n",
    "    ]),\n",
    "    \"target\": np.array([\n",
    "        [1, 0],  # Sample 1: positive for target 1, negative for target 2\n",
    "        [0, 1],  # Sample 2: negative for target 1, positive for target 2\n",
    "        # ... more samples\n",
    "    ]),\n",
    "    \"target_names\": [\"group_A\", \"group_B\"]  # Names for the two target columns\n",
    "}\n",
    "```\n",
    "\n",
    "## Uploading the dataset\n",
    "Once your data is prepared, use `upload_dataset()` to save it to MLflow.\n",
    "This will verify that the data is formatted correctly and then upload it to the server.\n",
    "\n",
    "```python\n",
    "upload_dataset(\n",
    "    data=data,\n",
    "    dataset_name=\"ftir_no_bonding_effects\",  # Name of the dataset category\n",
    "    version_name=\"initial_data\",  # Description of this version\n",
    "    description=\"FTIR dataset downloaded from the FCGFormer paper without any modifications\",\n",
    "\n",
    "    # The below have sensible default values to make the train/validation/test splits\n",
    "    # so they can be left unspecified\n",
    "    split=True,  # Whether to create train/valid/test splits (default: True)\n",
    "    test_size=0.2,  # Fraction of data for test set (default: 0.2)\n",
    "    valid_size=0.2,  # Fraction of remaining data for validation (default: 0.2)\n",
    "    random_state=42  # For reproducible splits (default: 42)\n",
    ")\n",
    "```\n",
    "\n",
    "By default, the dataset will be automatically split into train (64%), validation (16%), and test (20%) sets. \n",
    "Each split will have its own inputs, targets, and positive counts (number of samples with positive label) saved as separate files within the same MLflow run.\n",
    "\n",
    "## Accessing the Dataset\n",
    "\n",
    "After upload, the dataset will be available in MLflow for model training with:\n",
    "- NumPy arrays saved as `.npy` files (e.g., `train_inputs.npy`, `valid_inputs.npy`, `test_inputs.npy`)\n",
    "- Target names in `target_names.txt`\n",
    "- Positive counts for each split in files like `train_pos_counts.txt`\n",
    "- The code of this notebook saved for reproducibility\n",
    "\n",
    "You can view your dataset in MLflow by opening the link printed after calling `upload_dataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d458a14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell defines upload_dataset. You can ignore it.\n",
    "\n",
    "# Install required packages for `upload_dataset()` with quiet flag to reduce output\n",
    "%pip install -q numpy mlflow ipynbname requests tqdm scikit-learn scikit-multilearn\n",
    "\n",
    "import os\n",
    "import urllib.parse\n",
    "import jupyter_client\n",
    "from typing import Dict, Any\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "try:\n",
    "    import mlflow\n",
    "except ImportError:\n",
    "    %pip install -q mlflow\n",
    "    import mlflow\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "except ImportError:\n",
    "    %pip install -q requests\n",
    "    import requests\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "except ImportError:\n",
    "    %pip install -q numpy\n",
    "    import numpy as np\n",
    "\n",
    "try:\n",
    "    from notebook import notebookapp\n",
    "except ImportError:\n",
    "    from jupyter_server import serverapp as notebookapp\n",
    "\n",
    "# MLFlow creds\n",
    "MLFLOW_DOMAIN = \"mlflow.gritans.lv\"\n",
    "MLFLOW_USERNAME = \"data_user\"\n",
    "MLFLOW_PASSWORD = \"ais7Rah2foo0gee9\"\n",
    "MLFLOW_TRACKING_URI = f\"https://{MLFLOW_DOMAIN}\"\n",
    "\n",
    "parsed_uri = urllib.parse.urlparse(MLFLOW_TRACKING_URI)\n",
    "auth_uri = parsed_uri._replace(\n",
    "    netloc=f\"{urllib.parse.quote(MLFLOW_USERNAME)}:{urllib.parse.quote(MLFLOW_PASSWORD)}@{parsed_uri.netloc}\"\n",
    ").geturl()\n",
    "\n",
    "mlflow.set_tracking_uri(auth_uri)\n",
    "\n",
    "# Add this import at the top of the upload_dataset function cell\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def upload_dataset(\n",
    "    data: Dict[str, Any],\n",
    "    dataset_name: str,\n",
    "    version_name: str,\n",
    "    description: str | None = None,\n",
    "    split: bool = True,\n",
    "    test_size: float = 0.2,\n",
    "    valid_size: float = 0.2,\n",
    "    random_state: int = 42\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (Dict[str, Any]): Dictionary containing the dataset with keys:\n",
    "            - \"inputs\": NumPy array of shape (num_samples, num_input_features)\n",
    "            - \"target\": NumPy array of shape (num_samples, num_output_features)\n",
    "            - \"target_names\": List of target feature names, in the same order as the target array.\n",
    "        dataset_name (str): Name of the dataset.\n",
    "        version_name (str): A descriptive version name for the dataset. \n",
    "        description (str): An (optional) description of this dataset version.\n",
    "        split (bool): Whether to create train/valid/test splits.\n",
    "        test_size (float): Fraction of data for test set.\n",
    "        valid_size (float): Fraction of remaining data for validation.\n",
    "        random_state (int): For reproducible splits.\n",
    "    \"\"\"\n",
    "    print(f\"üöÄ Starting dataset upload process for '{dataset_name}' - {version_name}\")\n",
    "    \n",
    "    # Check dictionary\n",
    "    print(\"‚úÖ Validating dataset format...\")\n",
    "    expected_keys = {\"inputs\", \"target\", \"target_names\"}\n",
    "    assert set(data.keys()) == expected_keys, (\n",
    "        f\"Invalid dataset format. Keys should be {expected_keys}.\"\n",
    "    )\n",
    "\n",
    "    # Check expected types\n",
    "    assert isinstance(data[\"inputs\"], np.ndarray), (\n",
    "        f\"Inputs must be a numpy.ndarray. Got {type(data['inputs'])}.\"\n",
    "    )\n",
    "    assert isinstance(data[\"target\"], np.ndarray), (\n",
    "        f\"Targets must be a numpy.ndarray. Got {type(data['target'])}.\"\n",
    "    )\n",
    "    assert isinstance(data[\"target_names\"], list), (\n",
    "        f\"target names must be a list. Got {type(data['target_names'])}.\"\n",
    "    )\n",
    "    assert all(isinstance(name, str) for name in data[\"target_names\"]), (\n",
    "        \"All target names must be strings.\"\n",
    "    )\n",
    "\n",
    "    # Check expected shapes\n",
    "    inputs: np.ndarray = data[\"inputs\"]\n",
    "    target: np.ndarray = data[\"target\"]\n",
    "    target_names = data[\"target_names\"]\n",
    "\n",
    "    assert inputs.ndim == 2, (\n",
    "        f\"Inputs must be a (num_samples, num_input_features) array. \"\n",
    "        f\"Got {inputs.ndim} dimensions.\"\n",
    "    )\n",
    "    assert target.ndim == 2, (\n",
    "        f\"Targets must be a (num_samples, num_output_features) array. \"\n",
    "        f\"Got {target.ndim} dimensions.\"\n",
    "    )\n",
    "\n",
    "    n_samples = inputs.shape[0]\n",
    "    assert n_samples > 0, (\n",
    "        f\"Inputs must have at least one sample. Got {n_samples} samples.\"\n",
    "    )\n",
    "    assert n_samples == target.shape[0], (\n",
    "        f\"Inputs and targets must have the same number of samples. \"\n",
    "        f\"Got {n_samples} inputs and {target.shape[0]} targets.\"\n",
    "    )\n",
    "\n",
    "    n_outputs = target.shape[1]\n",
    "    assert n_outputs > 0 and n_outputs == len(target_names), (\n",
    "        f\"Targets must have the same number of features as target names. \"\n",
    "        f\"Got {n_outputs} target features and {len(target_names)} target names.\"\n",
    "    )\n",
    "\n",
    "    # Create train/valid/test splits if requested\n",
    "    splits = {}\n",
    "    \n",
    "    if split:\n",
    "        print(\"üî™ Creating train/valid/test splits...\")\n",
    "        \n",
    "        # For multi-label data, we need to use iterative stratification\n",
    "        print(\"  - Using iterative stratification for multi-label data\")\n",
    "        \n",
    "        # First split off the test set\n",
    "        train_valid_inputs, train_valid_target, test_inputs, test_target = iterative_train_test_split(\n",
    "            inputs, target, test_size=test_size\n",
    "        )\n",
    "        \n",
    "        # Split remaining data into train/valid\n",
    "        train_inputs, train_target, valid_inputs, valid_target = iterative_train_test_split(\n",
    "            train_valid_inputs, train_valid_target, test_size=valid_size\n",
    "        )\n",
    "        \n",
    "        # Store the splits\n",
    "        splits = {\n",
    "            \"train\": (train_inputs, train_target),\n",
    "            \"valid\": (valid_inputs, valid_target),\n",
    "            \"test\": (test_inputs, test_target)\n",
    "        }\n",
    "        \n",
    "        # Print split statistics\n",
    "        print(\"üìä Split statistics:\")\n",
    "        for split_name, (split_x, split_y) in splits.items():\n",
    "            split_size = len(split_x)\n",
    "            split_pct = split_size / n_samples * 100\n",
    "            print(f\"  - {split_name}: {split_size} samples ({split_pct:.1f}%)\")\n",
    "    else:\n",
    "        # Just use the original dataset\n",
    "        splits = {\"full\": (inputs, target)}\n",
    "    \n",
    "    # Connect to MLFlow\n",
    "    print(\"üîç Connecting to MLflow server...\")\n",
    "    mlflow.set_experiment(experiment_name=dataset_name)\n",
    "    with mlflow.start_run(run_name=version_name, description=description) as run:\n",
    "        print(f\"‚úÖ Created run with ID: {run.info.run_id}\")\n",
    "        local_dir = os.path.join(\"./runs\", run.info.run_id)\n",
    "        os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "        # Log the notebook generating this dataset\n",
    "        print(\"üìù Locating and saving notebook source...\")\n",
    "        try:\n",
    "            # primary: ipynbname often just works\n",
    "            import ipynbname\n",
    "            notebook_path = ipynbname.path()\n",
    "            print(f\"‚úÖ Found notebook at: {notebook_path}\")\n",
    "        except Exception:\n",
    "            # Fallback notebook location code - unchanged\n",
    "            print(\"‚ö†Ô∏è Primary notebook path detection failed, trying alternative method...\")\n",
    "            # ... existing fallback code ...\n",
    "            conn_file = jupyter_client.find_connection_file()\n",
    "            kernel_id = os.path.basename(conn_file).split('-', 1)[1].split('.')[0]\n",
    "\n",
    "            # 2) iterate over all running notebook servers\n",
    "            for srv in notebookapp.list_running_servers():\n",
    "                # build the URL for sessions\n",
    "                url = srv['url'].rstrip('/') + '/api/sessions'\n",
    "                token = srv.get('token', '')\n",
    "                params = {'token': token} if token else {}\n",
    "\n",
    "                try:\n",
    "                    resp = requests.get(url, params=params)\n",
    "                    resp.raise_for_status()\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                # 3) look for our kernel in their active sessions\n",
    "                for sess in resp.json():\n",
    "                    if sess['kernel']['id'] == kernel_id:\n",
    "                        # 4) reconstruct the full path\n",
    "                        rel_path = sess['notebook']['path']       # e.g. \"subdir/MyNotebook.ipynb\"\n",
    "                        notebook_path = os.path.join(srv['notebook_dir'], rel_path)\n",
    "                        print(f\"‚úÖ Found notebook at: {notebook_path}\")\n",
    "                        break\n",
    "                else:\n",
    "                    continue\n",
    "                break\n",
    "            else:\n",
    "                raise RuntimeError(\"Could not locate the current notebook path\")\n",
    "\n",
    "        mlflow.log_artifact(notebook_path)\n",
    "        \n",
    "        # Log dataset files - one set per split\n",
    "        print(\"üìÑ Preparing and uploading dataset files...\")\n",
    "        \n",
    "        target_names_path = os.path.join(local_dir, \"target_names.txt\")\n",
    "        with open(target_names_path, \"w\") as f:\n",
    "            for name in target_names:\n",
    "                f.write(f\"{name}\\n\")\n",
    "        \n",
    "        files_to_upload = [(\"target names\", target_names_path)]\n",
    "        \n",
    "        # Save each split with its own files\n",
    "        for split_name, (split_inputs, split_target) in splits.items():\n",
    "            print(f\"  - Processing {split_name} split...\")\n",
    "            \n",
    "            # Calculate positive counts for this split\n",
    "            split_pos_counts = split_target.sum(axis=0)\n",
    "            \n",
    "            # Save split-specific files\n",
    "            split_inputs_path = os.path.join(local_dir, f\"{split_name}_inputs.npy\")\n",
    "            split_target_path = os.path.join(local_dir, f\"{split_name}_target.npy\")\n",
    "            split_pos_counts_path = os.path.join(local_dir, f\"{split_name}_pos_counts.txt\")\n",
    "            \n",
    "            # Save numpy arrays\n",
    "            np.save(split_inputs_path, split_inputs)\n",
    "            np.save(split_target_path, split_target)\n",
    "            \n",
    "            # Save positive counts\n",
    "            with open(split_pos_counts_path, \"w\") as f:\n",
    "                for i, count in enumerate(split_pos_counts):\n",
    "                    f.write(f\"{target_names[i]}: {count}\\n\")\n",
    "            \n",
    "            # Add to upload list\n",
    "            files_to_upload.extend([\n",
    "                (f\"{split_name} inputs\", split_inputs_path),\n",
    "                (f\"{split_name} target\", split_target_path),\n",
    "                (f\"{split_name} pos counts\", split_pos_counts_path)\n",
    "            ])\n",
    "        \n",
    "        # Upload all files to MLFlow\n",
    "        print(\"üì§ Uploading files to MLflow server...\")\n",
    "        for desc, filepath in tqdm(files_to_upload, desc=\"Uploading files\"):\n",
    "            print(f\"  - Uploading {desc}...\")\n",
    "            mlflow.log_artifact(filepath)\n",
    "            time.sleep(0.2)  # Small delay for progress visibility\n",
    "\n",
    "        # Log metadata\n",
    "        print(\"üìä Logging metadata...\")\n",
    "        mlflow.log_param(\"target_names\", target_names)\n",
    "        mlflow.log_param(\"input_features\", inputs.shape[1])\n",
    "        mlflow.log_param(\"output_features\", target.shape[1])\n",
    "        mlflow.log_param(\"num_samples\", n_samples)\n",
    "        mlflow.log_param(\"splits\", list(splits.keys()))\n",
    "        \n",
    "        if description:\n",
    "            mlflow.set_tag(\"description\", description)\n",
    "        \n",
    "        print(f\"‚úÖ Dataset upload complete! View at: {MLFLOW_TRACKING_URI}/#/experiments/{run.info.experiment_id}/runs/{run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa7a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template cell - replace with your data processing code\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load/prepare your data\n",
    "inputs = None  # TODO: Replace with your feature matrix\n",
    "target = None  # TODO: Replace with your target matrix\n",
    "target_names = []  # TODO: Replace with your class names\n",
    "\n",
    "# Step 2: Package and upload\n",
    "data = {\n",
    "    \"inputs\": inputs,\n",
    "    \"target\": target,\n",
    "    \"target_names\": target_names\n",
    "}\n",
    "\n",
    "# Step 3: Upload with auto-splitting\n",
    "result = upload_dataset(\n",
    "    data=data,\n",
    "    dataset_name=\"your_dataset_name\",  # TODO: Set your dataset name\n",
    "    version_name=\"v1\",  # TODO: Set your version\n",
    "    description=\"Your dataset description\",  # Optional\n",
    "    split=True  # Creates train/valid/test splits\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4376590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_dataset(\n",
    "    data=data,\n",
    "    dataset_name = ...,\n",
    "    version_name = ..., \n",
    "    description = None # Optional\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
