{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8428da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't modify this cell\n",
    "\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import urllib.parse\n",
    "import os\n",
    "import numpy as np\n",
    "import ast\n",
    "# MLFlow creds\n",
    "MLFLOW_DOMAIN = \"https://mlflow.gritans.lv\"\n",
    "MLFLOW_USERNAME = \"data_user\"\n",
    "MLFLOW_PASSWORD = \"ais7Rah2foo0gee9\"\n",
    "MLFLOW_TRACKING_URI = f\"{MLFLOW_DOMAIN}\"\n",
    "\n",
    "parsed_uri = urllib.parse.urlparse(MLFLOW_TRACKING_URI)\n",
    "auth_uri = parsed_uri._replace(\n",
    "    netloc=f\"{urllib.parse.quote(MLFLOW_USERNAME)}:{urllib.parse.quote(MLFLOW_PASSWORD)}@{parsed_uri.netloc}\"\n",
    ").geturl()\n",
    "\n",
    "mlflow.set_tracking_uri(auth_uri)\n",
    "\n",
    "def upload_dataset(\n",
    "    train_df: pd.DataFrame,\n",
    "    valid_df: pd.DataFrame,\n",
    "    dataset_name: str,\n",
    "    version_name: str,\n",
    "    description: str | None = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        train_df (pd.DataFrame): DataFrame containing the training data.\n",
    "        valid_df (pd.DataFrame): DataFrame containing the validation data.\n",
    "        dataset_name (str): Name of the dataset to be used in MLFlow.\n",
    "        version_name (str): Version name for the dataset.\n",
    "        description (str | None): Description of the dataset. Default is None.\n",
    "    \"\"\"\n",
    "    \n",
    "    for df in [train_df, valid_df]:\n",
    "        assert isinstance(df, pd.DataFrame), \"df must be a pandas DataFrame\"\n",
    "        assert not df.empty, \"df must not be empty\"\n",
    "        assert not df.isnull().values.any(), \"df must not contain NaN values\"\n",
    "        assert \"spectrum\" in df.columns, \"df must contain a 'spectrum' column\"\n",
    "        assert isinstance(df[\"spectrum\"].iloc[0], np.ndarray), \"spectrum column must contain numpy arrays\"\n",
    "        assert df[\"spectrum\"].iloc[0].ndim == 1, \"spectrum column must contain 1D numpy arrays\"\n",
    "        assert df[\"spectrum\"].iloc[0].dtype in [float, np.float32, np.float64], \"spectrum column must contain float values\"\n",
    "\n",
    "\n",
    "    mlflow.set_experiment(experiment_name=dataset_name)\n",
    "    with mlflow.start_run(run_name=version_name, description=description) as run:\n",
    "        local_dir = os.path.join(\"./runs\", run.info.run_id)\n",
    "        os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "        # Log train and valid DataFrames\n",
    "        train_path = os.path.join(local_dir, \"train_df.csv.gz\")\n",
    "        valid_path = os.path.join(local_dir, \"valid_df.csv.gz\")\n",
    "\n",
    "        # map to list\n",
    "        train_df[\"spectrum\"] = train_df[\"spectrum\"].apply(lambda x: x.tolist())\n",
    "        valid_df[\"spectrum\"] = valid_df[\"spectrum\"].apply(lambda x: x.tolist())\n",
    "\n",
    "        # save as csv\n",
    "        train_df.to_csv(train_path, index=False, compression='gzip')\n",
    "        valid_df.to_csv(valid_path, index=False, compression='gzip')\n",
    "        \n",
    "        mlflow.log_artifact(train_path)\n",
    "        mlflow.log_artifact(valid_path)\n",
    "\n",
    "        # Log metadata\n",
    "        mlflow.log_param(\"train_size\", len(train_df))\n",
    "        mlflow.log_param(\"valid_size\", len(valid_df))\n",
    "        mlflow.log_param(\"spectrum_len\", len(train_df[\"spectrum\"].iloc[0]))\n",
    "        \n",
    "        # bool_column: num_positive\n",
    "        for split_name, split_df in zip([\"train\", \"valid\"], [train_df, valid_df]):\n",
    "            pos_counts = {}\n",
    "            for col in split_df.columns:\n",
    "                if split_df[col].dtype == bool:\n",
    "                    pos_counts[col] = split_df[col].sum()\n",
    "            mlflow.log_param(f\"{split_name}_pos\", pos_counts)\n",
    "\n",
    "        # df head txt artifacts\n",
    "        train_head_path = os.path.join(local_dir, \"train_df_head.txt\")\n",
    "        valid_head_path = os.path.join(local_dir, \"valid_df_head.txt\")\n",
    "\n",
    "        with open(train_head_path, \"w\") as f:\n",
    "            f.write(str(train_df.head(n=5)))\n",
    "        with open(valid_head_path, \"w\") as f:\n",
    "            f.write(str(valid_df.head(n=5)))\n",
    "\n",
    "        mlflow.log_artifact(train_head_path)\n",
    "        mlflow.log_artifact(valid_head_path)\n",
    "\n",
    "        if description:\n",
    "            mlflow.set_tag(\"description\", description)\n",
    "\n",
    "def load_ftir() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Downloads the non-augmented FTIR dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    run_id = \"f97846a98e434a5e907d6abce6ee1916\"\n",
    "    artifact = \"FTIR_split.csv\"\n",
    "    local_path = \"./tmp/\"\n",
    "    df_path = os.path.join(local_path, artifact)\n",
    "\n",
    "    if not os.path.exists(df_path):\n",
    "        print(f\"Artifact {artifact} not found in local path. Downloading...\")\n",
    "        os.makedirs(local_path, exist_ok=True)\n",
    "        mlflow.artifacts.download_artifacts(run_id=run_id, artifact_path=artifact, dst_path=local_path) # type: ignore\n",
    "\n",
    "    df = pd.read_csv(df_path)\n",
    "    str_to_arr = lambda st: np.array(ast.literal_eval(st), dtype=np.float32)\n",
    "    df[\"spectrum\"] = df[\"spectrum\"].apply(str_to_arr) # type: ignore\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95175abc",
   "metadata": {},
   "source": [
    "## 1. download the original NIST dataset\n",
    "This will take some time to decode csv.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6204fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this downloads the non-augmented FTIR dataset\n",
    "original_df = load_ftir() \n",
    "original_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef891d5",
   "metadata": {},
   "source": [
    "## 2. add extra data (e.g. chemmotion and graphformer)\n",
    "Add chemmotion/graphformer samples and mark them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a526c6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "nist_df = original_df.copy()\n",
    "nist_df[\"source\"] = \"nist\"\n",
    "\n",
    "# Chemmotion\n",
    "chemmotion_df = original_df.iloc[0:0].copy() # this creates an empty DataFrame with the same columns\n",
    "\n",
    "# TODO load chemmotion\n",
    "\n",
    "chemmotion_df[\"source\"] = \"chemmotion\"\n",
    "chemmotion_df[\"is_train\"] = True\n",
    "\n",
    "# Graphgormer\n",
    "graphgormer_df = original_df.iloc[0:0].copy()\n",
    "\n",
    "# TODO load graphgormer\n",
    "\n",
    "graphgormer_df[\"source\"] = \"graphformer\"\n",
    "graphgormer_df[\"is_train\"] = True\n",
    "\n",
    "combined_df = pd.concat([nist_df, chemmotion_df, graphgormer_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdbb341",
   "metadata": {},
   "source": [
    "## 3. add extra columns to the dataset\n",
    "add columns for hydrogen bonding info, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa852db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_df = combined_df.copy()\n",
    "# TODO add extra columns to extra_df\n",
    "extra_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a5763e",
   "metadata": {},
   "source": [
    "## 4. split the dataset into training and validation parts\n",
    "We split off the validation dataset after adding extra columns, but before creating new samples (augmenting) because we want to test the model on the real samples, not artifical ones created by e.g. LSER.\n",
    "\n",
    "Augmented samples are only meant to improve training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfacb709",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# need to specify which columns are the actual targets we want to predict\n",
    "# to make it so that the distribution of the training and validation sets is similar\n",
    "\n",
    "# this can probably be just left to the functional group names\n",
    "\n",
    "# because even though we also predict e.g. hydrogen bonding (so that the model has more to learn)\n",
    "# it is not what we are actually interested in\n",
    "\n",
    "target_names = ['alkane', 'methyl', 'alkene', 'alkyne', 'alcohols', 'amines',\n",
    "                'nitriles', 'aromatics', 'alkyl halides', 'esters', 'ketones', \n",
    "                'aldehydes', 'carboxylic acids', 'ether', 'acyl halides', \n",
    "                'amides', 'nitro']\n",
    "\n",
    "# split based on is_train column\n",
    "train_df = extra_df[extra_df[\"is_train\"] == True].copy()\n",
    "valid_df = extra_df[extra_df[\"is_train\"] == False].copy()\n",
    "\n",
    "# Optional: Check label distribution balance\n",
    "print(\"\\nLabel distribution comparison:\")\n",
    "for col in target_names:\n",
    "    train_ratio = train_df[col].mean()\n",
    "    valid_ratio = valid_df[col].mean()\n",
    "    print(f\"{col}: Train={train_ratio:.3f}, Valid={valid_ratio:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834b1418",
   "metadata": {},
   "source": [
    "## 5. augment the training dataset\n",
    "Here we add extra samples via  LSER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a5d7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from LSER_augment2 import debug_apply_lser_shifts\n",
    "from tqdm import tqdm\n",
    "\n",
    "pi_stars = [-0.33, -0.08, 0.14, 0.27, 0.59, 0.71, 0.88, 1.09]\n",
    "betas = [0.00, 0.00, 0.71, 0.49, 0.18, 0.48, 0.76, 0.31 ]\n",
    "alphas = [0.00, 0.00, 0.00, 0.00, 0.78, 0.08, 0.00, 1.17 ]\n",
    "wavenumbers = np.linspace(400,4001,3602)\n",
    "\n",
    "new_rows = []\n",
    "\n",
    "for i in tqdm(range(len(train_df))):\n",
    "    row = train_df.iloc[i].copy() \n",
    "    row[\"lser\"] = False # original sample\n",
    "\n",
    "    fgs = ['alcohols', 'ketones', 'aldehydes', 'esters', 'amides', 'nitriles', 'carboxylic acids', 'alkyl halides', 'nitro']\n",
    "\n",
    "    if any(row[fg] for fg in fgs):\n",
    "        for i in range(0,8):\n",
    "            pi_star = pi_stars[i]\n",
    "            beta = betas[i]\n",
    "            alpha = alphas[i]\n",
    "            row_copy = row.copy()\n",
    "            spectrum = row_copy[\"spectrum\"]\n",
    "            functional_groups = {fg: row_copy[fg] for fg in fgs}\n",
    "            row_copy['spectrum'], _ = debug_apply_lser_shifts(\n",
    "                spectrum, wavenumbers, functional_groups, pi_star, beta, alpha\n",
    "            )\n",
    "            row_copy[\"lser\"] = True\n",
    "            new_rows.append(row_copy)            \n",
    "\n",
    "    new_rows.append(row)\n",
    "\n",
    "augmented_train_df = pd.DataFrame(new_rows)\n",
    "augmented_train_df.reset_index(drop=True, inplace=True)\n",
    "augmented_train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7f01b1",
   "metadata": {},
   "source": [
    "## 6. upload the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2aea37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload_dataset(\n",
    "#     train_df=augmented_train_df,\n",
    "#     valid_df=valid_df,\n",
    "#     dataset_name=\"dataset_FTIR_example_csv\",\n",
    "#     version_name=\"v1\",\n",
    "#     description=\"Example unchanged dataset\"\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
