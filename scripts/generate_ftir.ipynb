{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1466c028",
   "metadata": {},
   "source": [
    "%%markdown\n",
    "# Dataset Generation Template\n",
    "\n",
    "This notebook provides a template for generating and uploading new datasets to MLflow, after you have the raw scraped data.\n",
    "\n",
    "## Data Format\n",
    "\n",
    "Your data needs to be organized in a dictionary with three required keys:\n",
    "- `inputs`: Feature matrix as NumPy array\n",
    "- `target`: Binary target matrix as NumPy array\n",
    "- `target_names`: List of strings naming each target feature\n",
    "\n",
    "### Shape Requirements\n",
    "\n",
    "| Component | Type | Shape | Description | Example |\n",
    "|-----------|------|-------|-------------|----------|\n",
    "| `inputs` | `np.ndarray` | `(n_samples, n_features)` | Each row is one sample, each column a feature | `(1000, 10)` for 1000 samples with 10 features |\n",
    "| `target` | `np.ndarray` | `(n_samples, n_targets)` | Binary matrix where each column represents one target | `(1000, 3)` for 1000 samples with 3 possible targets |\n",
    "| `target_names` | `list[str]` | `(n_targets,)` | Names for each target column | `[\"cat\", \"dog\", \"bird\"]` for 3 targets |\n",
    "\n",
    "## Example\n",
    "\n",
    "```python\n",
    "data = {\n",
    "    \"inputs\": np.array([\n",
    "        [0.1, 0.2, 0.3],  # Sample 1 with 3 features\n",
    "        [0.4, 0.5, 0.6],  # Sample 2 with 3 features\n",
    "        # ... more samples\n",
    "    ]),\n",
    "    \"target\": np.array([\n",
    "        [1, 0],  # Sample 1: positive for target 1, negative for target 2\n",
    "        [0, 1],  # Sample 2: negative for target 1, positive for target 2\n",
    "        # ... more samples\n",
    "    ]),\n",
    "    \"target_names\": [\"group_A\", \"group_B\"]  # Names for the two target columns\n",
    "}\n",
    "```\n",
    "\n",
    "## Uploading the dataset\n",
    "Once your data is prepared, use `upload_dataset()` to save it to MLflow.\n",
    "This will verify that the data is formatted correctly and then upload it to the server.\n",
    "\n",
    "```python\n",
    "upload_dataset(\n",
    "    data=data,\n",
    "    dataset_name=\"ftir_no_bonding_effects\",  # Broader dataest for which we can have multiple versions\n",
    "    version_name=\"initial_data\",  # Description of this version\n",
    "    description=\"FTIR dataset downloaded from the FCGFormer paper without any modifications\"  # Optional details\n",
    ")\n",
    "```\n",
    "\n",
    "## Accessing the Dataset\n",
    "\n",
    "After upload, the dataset will be available in MLflow for model training with:\n",
    "- NumPy arrays saved as `.npy` files\n",
    "- Target names and counts (number of positive examples) in text files\n",
    "- The code of this notebook saved for reproducability (so you don't have to upload it anywhere)\n",
    "\n",
    "You can view your dataset in MLflow by opening the link printed after calling `upload_dataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d458a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-2.2.5-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting mlflow\n",
      "  Using cached mlflow-2.22.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting ipynbname\n",
      "  Using cached ipynbname-2024.1.0.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: requests in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (2.32.3)\n",
      "Requirement already satisfied: tqdm in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (4.66.5)\n",
      "Collecting mlflow-skinny==2.22.0 (from mlflow)\n",
      "  Using cached mlflow_skinny-2.22.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting Flask<4 (from mlflow)\n",
      "  Using cached flask-3.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: Jinja2<4,>=2.11 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from mlflow) (3.1.4)\n",
      "Collecting alembic!=1.10.0,<2 (from mlflow)\n",
      "  Using cached alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting docker<8,>=4.0.0 (from mlflow)\n",
      "  Using cached docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting graphene<4 (from mlflow)\n",
      "  Using cached graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting gunicorn<24 (from mlflow)\n",
      "  Using cached gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting markdown<4,>=3.3 (from mlflow)\n",
      "  Using cached markdown-3.8-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting matplotlib<4 (from mlflow)\n",
      "  Downloading matplotlib-3.10.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (11 kB)\n",
      "Collecting pandas<3 (from mlflow)\n",
      "  Using cached pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Collecting pyarrow<20,>=4.0.0 (from mlflow)\n",
      "  Using cached pyarrow-19.0.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting scikit-learn<2 (from mlflow)\n",
      "  Using cached scikit_learn-1.6.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Collecting scipy<2 (from mlflow)\n",
      "  Downloading scipy-1.15.3-cp312-cp312-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Collecting sqlalchemy<3,>=1.4.0 (from mlflow)\n",
      "  Using cached sqlalchemy-2.0.40-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
      "Collecting cachetools<6,>=5.0.0 (from mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached cachetools-5.5.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting click<9,>=7.0 (from mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting cloudpickle<4 (from mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached databricks_sdk-0.52.0-py3-none-any.whl.metadata (39 kB)\n",
      "Collecting fastapi<1 (from mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting gitpython<4,>=3.1.9 (from mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached GitPython-3.1.44-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting importlib_metadata!=4.7.0,<9,>=3.7.0 (from mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting opentelemetry-api<3,>=1.9.0 (from mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached opentelemetry_api-1.33.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting opentelemetry-sdk<3,>=1.9.0 (from mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached opentelemetry_sdk-1.33.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: packaging<25 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from mlflow-skinny==2.22.0->mlflow) (24.1)\n",
      "Collecting protobuf<7,>=3.12.0 (from mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached protobuf-6.30.2-cp39-abi3-macosx_10_9_universal2.whl.metadata (593 bytes)\n",
      "Collecting pydantic<3,>=1.10.8 (from mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached pydantic-2.11.4-py3-none-any.whl.metadata (66 kB)\n",
      "Requirement already satisfied: pyyaml<7,>=5.1 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from mlflow-skinny==2.22.0->mlflow) (6.0.2)\n",
      "Collecting sqlparse<1,>=0.4.0 (from mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached sqlparse-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting typing-extensions<5,>=4.0.0 (from mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting uvicorn<1 (from mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: ipykernel in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from ipynbname) (6.29.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from requests) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from requests) (2024.8.30)\n",
      "Collecting Mako (from alembic!=1.10.0,<2->mlflow)\n",
      "  Using cached mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting Werkzeug>=3.1 (from Flask<4->mlflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting itsdangerous>=2.2 (from Flask<4->mlflow)\n",
      "  Using cached itsdangerous-2.2.0-py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting blinker>=1.9 (from Flask<4->mlflow)\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Using cached graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
      "  Using cached graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from Jinja2<4,>=2.11->mlflow) (3.0.2)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib<4->mlflow)\n",
      "  Using cached contourpy-1.3.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib<4->mlflow)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib<4->mlflow)\n",
      "  Using cached fonttools-4.57.0-cp312-cp312-macosx_10_13_universal2.whl.metadata (102 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib<4->mlflow)\n",
      "  Using cached kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.2 kB)\n",
      "Collecting pillow>=8 (from matplotlib<4->mlflow)\n",
      "  Using cached pillow-11.2.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (8.9 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib<4->mlflow)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting pytz>=2020.1 (from pandas<3->mlflow)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas<3->mlflow)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn<2->mlflow)\n",
      "  Using cached joblib-1.5.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn<2->mlflow)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: appnope in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from ipykernel->ipynbname) (0.1.4)\n",
      "Requirement already satisfied: comm>=0.1.1 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from ipykernel->ipynbname) (0.2.2)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from ipykernel->ipynbname) (1.8.8)\n",
      "Requirement already satisfied: ipython>=7.23.1 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from ipykernel->ipynbname) (8.29.0)\n",
      "Requirement already satisfied: jupyter-client>=6.1.12 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from ipykernel->ipynbname) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from ipykernel->ipynbname) (5.7.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from ipykernel->ipynbname) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from ipykernel->ipynbname) (1.6.0)\n",
      "Requirement already satisfied: psutil in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from ipykernel->ipynbname) (6.1.0)\n",
      "Requirement already satisfied: pyzmq>=24 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from ipykernel->ipynbname) (26.2.0)\n",
      "Requirement already satisfied: tornado>=6.1 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from ipykernel->ipynbname) (6.4.1)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from ipykernel->ipynbname) (5.14.3)\n",
      "Collecting google-auth~=2.0 (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached google_auth-2.40.1-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting starlette<0.47.0,>=0.40.0 (from fastapi<1->mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython<4,>=3.1.9->mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting zipp>=3.20 (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached zipp-3.21.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: decorator in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->ipynbname) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->ipynbname) (0.19.1)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->ipynbname) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->ipynbname) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->ipynbname) (0.6.3)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from ipython>=7.23.1->ipykernel->ipynbname) (4.9.0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->ipynbname) (3.10.0)\n",
      "Collecting deprecated>=1.2.6 (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting importlib_metadata!=4.7.0,<9,>=3.7.0 (from mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached importlib_metadata-8.6.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.54b0 (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached opentelemetry_semantic_conventions-0.54b0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.16.0)\n",
      "Requirement already satisfied: h11>=0.8 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from uvicorn<1->mlflow-skinny==2.22.0->mlflow) (0.14.0)\n",
      "Collecting wrapt<2,>=1.10 (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached wrapt-1.17.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.4 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel->ipynbname) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel->ipynbname) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel->ipynbname) (0.2.13)\n",
      "Requirement already satisfied: anyio<5,>=3.6.2 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==2.22.0->mlflow) (4.6.2.post1)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel->ipynbname) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel->ipynbname) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from stack-data->ipython>=7.23.1->ipykernel->ipynbname) (0.2.3)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/jekabsgritans/miniconda3/lib/python3.12/site-packages (from anyio<5,>=3.6.2->starlette<0.47.0,>=0.40.0->fastapi<1->mlflow-skinny==2.22.0->mlflow) (1.3.1)\n",
      "Collecting pyasn1<0.7.0,>=0.6.1 (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Using cached numpy-2.2.5-cp312-cp312-macosx_14_0_arm64.whl (5.2 MB)\n",
      "Using cached mlflow-2.22.0-py3-none-any.whl (29.0 MB)\n",
      "Using cached mlflow_skinny-2.22.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached ipynbname-2024.1.0.0-py3-none-any.whl (4.3 kB)\n",
      "Using cached alembic-1.15.2-py3-none-any.whl (231 kB)\n",
      "Using cached docker-7.1.0-py3-none-any.whl (147 kB)\n",
      "Using cached flask-3.1.0-py3-none-any.whl (102 kB)\n",
      "Using cached graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
      "Using cached gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
      "Using cached markdown-3.8-py3-none-any.whl (106 kB)\n",
      "Downloading matplotlib-3.10.3-cp312-cp312-macosx_11_0_arm64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pandas-2.2.3-cp312-cp312-macosx_11_0_arm64.whl (11.4 MB)\n",
      "Using cached pyarrow-19.0.1-cp312-cp312-macosx_12_0_arm64.whl (30.7 MB)\n",
      "Using cached scikit_learn-1.6.1-cp312-cp312-macosx_12_0_arm64.whl (11.2 MB)\n",
      "Downloading scipy-1.15.3-cp312-cp312-macosx_14_0_arm64.whl (22.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.4/22.4 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sqlalchemy-2.0.40-cp312-cp312-macosx_11_0_arm64.whl (2.1 MB)\n",
      "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Using cached cachetools-5.5.2-py3-none-any.whl (10 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Using cached cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Using cached contourpy-1.3.2-cp312-cp312-macosx_11_0_arm64.whl (255 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached databricks_sdk-0.52.0-py3-none-any.whl (700 kB)\n",
      "Using cached fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
      "Using cached fonttools-4.57.0-cp312-cp312-macosx_10_13_universal2.whl (2.8 MB)\n",
      "Using cached GitPython-3.1.44-py3-none-any.whl (207 kB)\n",
      "Using cached graphql_core-3.2.6-py3-none-any.whl (203 kB)\n",
      "Using cached graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
      "Using cached itsdangerous-2.2.0-py3-none-any.whl (16 kB)\n",
      "Using cached joblib-1.5.0-py3-none-any.whl (307 kB)\n",
      "Using cached kiwisolver-1.4.8-cp312-cp312-macosx_11_0_arm64.whl (65 kB)\n",
      "Using cached opentelemetry_api-1.33.0-py3-none-any.whl (65 kB)\n",
      "Using cached importlib_metadata-8.6.1-py3-none-any.whl (26 kB)\n",
      "Using cached opentelemetry_sdk-1.33.0-py3-none-any.whl (118 kB)\n",
      "Using cached opentelemetry_semantic_conventions-0.54b0-py3-none-any.whl (194 kB)\n",
      "Using cached pillow-11.2.1-cp312-cp312-macosx_11_0_arm64.whl (3.0 MB)\n",
      "Using cached protobuf-6.30.2-cp39-abi3-macosx_10_9_universal2.whl (417 kB)\n",
      "Using cached pydantic-2.11.4-py3-none-any.whl (443 kB)\n",
      "Using cached pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached sqlparse-0.5.3-py3-none-any.whl (44 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached mako-1.3.10-py3-none-any.whl (78 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Using cached google_auth-2.40.1-py2.py3-none-any.whl (216 kB)\n",
      "Using cached starlette-0.46.2-py3-none-any.whl (72 kB)\n",
      "Using cached typing_inspection-0.4.0-py3-none-any.whl (14 kB)\n",
      "Using cached zipp-3.21.0-py3-none-any.whl (9.6 kB)\n",
      "Using cached pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Using cached rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Using cached wrapt-1.17.2-cp312-cp312-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: pytz, zipp, wrapt, Werkzeug, tzdata, typing-extensions, threadpoolctl, sqlparse, smmap, pyparsing, pyasn1, pyarrow, protobuf, pillow, numpy, markdown, Mako, kiwisolver, joblib, itsdangerous, gunicorn, graphql-core, fonttools, cycler, cloudpickle, click, cachetools, blinker, annotated-types, uvicorn, typing-inspection, starlette, sqlalchemy, scipy, rsa, pydantic-core, pyasn1-modules, pandas, importlib_metadata, graphql-relay, gitdb, Flask, docker, deprecated, contourpy, scikit-learn, pydantic, opentelemetry-api, matplotlib, graphene, google-auth, gitpython, alembic, opentelemetry-semantic-conventions, fastapi, databricks-sdk, opentelemetry-sdk, ipynbname, mlflow-skinny, mlflow\n",
      "Successfully installed Flask-3.1.0 Mako-1.3.10 Werkzeug-3.1.3 alembic-1.15.2 annotated-types-0.7.0 blinker-1.9.0 cachetools-5.5.2 click-8.1.8 cloudpickle-3.1.1 contourpy-1.3.2 cycler-0.12.1 databricks-sdk-0.52.0 deprecated-1.2.18 docker-7.1.0 fastapi-0.115.12 fonttools-4.57.0 gitdb-4.0.12 gitpython-3.1.44 google-auth-2.40.1 graphene-3.4.3 graphql-core-3.2.6 graphql-relay-3.2.0 gunicorn-23.0.0 importlib_metadata-8.6.1 ipynbname-2024.1.0.0 itsdangerous-2.2.0 joblib-1.5.0 kiwisolver-1.4.8 markdown-3.8 matplotlib-3.10.3 mlflow-2.22.0 mlflow-skinny-2.22.0 numpy-2.2.5 opentelemetry-api-1.33.0 opentelemetry-sdk-1.33.0 opentelemetry-semantic-conventions-0.54b0 pandas-2.2.3 pillow-11.2.1 protobuf-6.30.2 pyarrow-19.0.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.11.4 pydantic-core-2.33.2 pyparsing-3.2.3 pytz-2025.2 rsa-4.9.1 scikit-learn-1.6.1 scipy-1.15.3 smmap-5.0.2 sqlalchemy-2.0.40 sqlparse-0.5.3 starlette-0.46.2 threadpoolctl-3.6.0 typing-extensions-4.13.2 typing-inspection-0.4.0 tzdata-2025.2 uvicorn-0.34.2 wrapt-1.17.2 zipp-3.21.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# This cell defines upload_dataset. You can ignore it.\n",
    "\n",
    "# Install required packages for `upload_dataset()` with quiet flag to reduce output\n",
    "%pip install -q numpy mlflow ipynbname requests tqdm\n",
    "\n",
    "import os\n",
    "import urllib.parse\n",
    "import jupyter_client\n",
    "from typing import Dict, Any\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "try:\n",
    "    import mlflow\n",
    "except ImportError:\n",
    "    %pip install -q mlflow\n",
    "    import mlflow\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "except ImportError:\n",
    "    %pip install -q requests\n",
    "    import requests\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "except ImportError:\n",
    "    %pip install -q numpy\n",
    "    import numpy as np\n",
    "\n",
    "try:\n",
    "    from notebook import notebookapp\n",
    "except ImportError:\n",
    "    from jupyter_server import serverapp as notebookapp\n",
    "\n",
    "# MLFlow creds\n",
    "MLFLOW_DOMAIN = \"mlflow.gritans.lv\"\n",
    "MLFLOW_USERNAME = \"data_user\"\n",
    "MLFLOW_PASSWORD = \"ais7Rah2foo0gee9\"\n",
    "MLFLOW_TRACKING_URI = f\"https://{MLFLOW_DOMAIN}\"\n",
    "\n",
    "parsed_uri = urllib.parse.urlparse(MLFLOW_TRACKING_URI)\n",
    "auth_uri = parsed_uri._replace(\n",
    "    netloc=f\"{urllib.parse.quote(MLFLOW_USERNAME)}:{urllib.parse.quote(MLFLOW_PASSWORD)}@{parsed_uri.netloc}\"\n",
    ").geturl()\n",
    "\n",
    "mlflow.set_tracking_uri(auth_uri)\n",
    "\n",
    "\n",
    "def upload_dataset(\n",
    "    data: Dict[str, Any],\n",
    "    dataset_name: str,\n",
    "    version_name: str,\n",
    "    description: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (Dict[str, Any]): Dictionary containing the dataset with keys:\n",
    "            - \"inputs\": NumPy array of shape (num_samples, num_input_features)\n",
    "            - \"target\": NumPy array of shape (num_samples, num_output_features)\n",
    "            - \"target_names\": List of target feature names, in the same order as the target array.\n",
    "        dataset_name (str): Name of the dataset.\n",
    "        version_name (str): A descriptive version name for the dataset. Doesn't need to be unique, just for reference.\n",
    "        description (str): An (optional) description of this dataset version.\n",
    "    \"\"\"\n",
    "    print(f\"🚀 Starting dataset upload process for '{dataset_name}' - {version_name}\")\n",
    "    \n",
    "    # Check dictionary\n",
    "    print(\"✅ Validating dataset format...\")\n",
    "    expected_keys = {\"inputs\", \"target\", \"target_names\"}\n",
    "    assert set(data.keys()) == expected_keys, (\n",
    "        f\"Invalid dataset format. Keys should be {expected_keys}.\"\n",
    "    )\n",
    "\n",
    "    # Check expected types\n",
    "    assert isinstance(data[\"inputs\"], np.ndarray), (\n",
    "        f\"Inputs must be a numpy.ndarray. Got {type(data['inputs'])}.\"\n",
    "    )\n",
    "    assert isinstance(data[\"target\"], np.ndarray), (\n",
    "        f\"Targets must be a numpy.ndarray. Got {type(data['target'])}.\"\n",
    "    )\n",
    "    assert isinstance(data[\"target_names\"], list), (\n",
    "        f\"target names must be a list. Got {type(data['target_names'])}.\"\n",
    "    )\n",
    "    assert all(isinstance(name, str) for name in data[\"target_names\"]), (\n",
    "        \"All target names must be strings.\"\n",
    "    )\n",
    "\n",
    "    # Check expected shapes\n",
    "    inputs: np.ndarray = data[\"inputs\"]\n",
    "    target: np.ndarray = data[\"target\"]\n",
    "    target_names = data[\"target_names\"]\n",
    "\n",
    "    assert inputs.ndim == 2, (\n",
    "        f\"Inputs must be a (num_samples, num_input_features) array. \"\n",
    "        f\"Got {inputs.ndim} dimensions.\"\n",
    "    )\n",
    "    assert target.ndim == 2, (\n",
    "        f\"Targets must be a (num_samples, num_output_features) array. \"\n",
    "        f\"Got {target.ndim} dimensions.\"\n",
    "    )\n",
    "\n",
    "    n_samples = inputs.shape[0]\n",
    "    assert n_samples > 0, (\n",
    "        f\"Inputs must have at least one sample. Got {n_samples} samples.\"\n",
    "    )\n",
    "    assert n_samples == target.shape[0], (\n",
    "        f\"Inputs and targets must have the same number of samples. \"\n",
    "        f\"Got {n_samples} inputs and {target.shape[0]} targets.\"\n",
    "    )\n",
    "\n",
    "    n_outputs = target.shape[1]\n",
    "    assert n_outputs > 0 and n_outputs == len(target_names), (\n",
    "        f\"Targets must have the same number of features as target names. \"\n",
    "        f\"Got {n_outputs} target features and {len(target_names)} target names.\"\n",
    "    )\n",
    "\n",
    "    # Compute number of positive samples per target\n",
    "    pos_counts = target.sum(axis=0)\n",
    "    \n",
    "    print(f\"📊 Dataset stats: {n_samples} samples, {inputs.shape[1]} input features, {n_outputs} target classes\")\n",
    "    \n",
    "    print(\"🔍 Connecting to MLflow server...\")\n",
    "    mlflow.set_experiment(experiment_name=dataset_name)\n",
    "    with mlflow.start_run(run_name=version_name) as run:\n",
    "        print(f\"✅ Created run with ID: {run.info.run_id}\")\n",
    "        local_dir = os.path.join(\"./runs\", run.info.run_id)\n",
    "        os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "        # Log the notebook generating this dataset\n",
    "        print(\"📝 Locating and saving notebook source...\")\n",
    "        try:\n",
    "            # primary: ipynbname often just works\n",
    "            import ipynbname\n",
    "            notebook_path = ipynbname.path()\n",
    "            print(f\"✅ Found notebook at: {notebook_path}\")\n",
    "\n",
    "        except Exception:\n",
    "            print(\"⚠️ Primary notebook path detection failed, trying alternative method...\")\n",
    "            # fallback: query the Jupyter server's /api/sessions\n",
    "            # 1) get your kernel id\n",
    "            conn_file = jupyter_client.find_connection_file()\n",
    "            kernel_id = os.path.basename(conn_file).split('-', 1)[1].split('.')[0]\n",
    "\n",
    "            # 2) iterate over all running notebook servers\n",
    "            for srv in notebookapp.list_running_servers():\n",
    "                # build the URL for sessions\n",
    "                url = srv['url'].rstrip('/') + '/api/sessions'\n",
    "                token = srv.get('token', '')\n",
    "                params = {'token': token} if token else {}\n",
    "\n",
    "                try:\n",
    "                    resp = requests.get(url, params=params)\n",
    "                    resp.raise_for_status()\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                # 3) look for our kernel in their active sessions\n",
    "                for sess in resp.json():\n",
    "                    if sess['kernel']['id'] == kernel_id:\n",
    "                        # 4) reconstruct the full path\n",
    "                        rel_path = sess['notebook']['path']       # e.g. \"subdir/MyNotebook.ipynb\"\n",
    "                        notebook_path = os.path.join(srv['notebook_dir'], rel_path)\n",
    "                        print(f\"✅ Found notebook at: {notebook_path}\")\n",
    "                        break\n",
    "                else:\n",
    "                    continue\n",
    "                break\n",
    "            else:\n",
    "                raise RuntimeError(\"Could not locate the current notebook path\")\n",
    "\n",
    "        mlflow.log_artifact(notebook_path)\n",
    "        print(\"✅ Notebook logged successfully\")\n",
    "\n",
    "        # Log the dataset\n",
    "        print(\"💾 Saving dataset files locally...\")\n",
    "        inputs_path = os.path.join(local_dir, \"inputs.npy\")\n",
    "        target_path = os.path.join(local_dir, \"target.npy\")\n",
    "        target_names_path = os.path.join(local_dir, \"target_names.txt\")\n",
    "        pos_counts_path = os.path.join(local_dir, \"pos_counts.txt\")\n",
    "\n",
    "        ## save locally\n",
    "        print(f\"  - Saving inputs array ({inputs.shape})...\")\n",
    "        np.save(inputs_path, inputs)\n",
    "        print(f\"  - Saving target array ({target.shape})...\")\n",
    "        np.save(target_path, target)\n",
    "\n",
    "        print(\"  - Saving target names...\")\n",
    "        with open(target_names_path, \"w\") as f:\n",
    "            for name in target_names:\n",
    "                f.write(f\"{name}\\n\")\n",
    "\n",
    "        print(\"  - Saving positive counts...\")\n",
    "        with open(pos_counts_path, \"w\") as f:\n",
    "            for i, count in enumerate(pos_counts):\n",
    "                f.write(f\"{target_names[i]}: {count}\\n\")\n",
    "\n",
    "        ## upload to mlflow\n",
    "        print(\"📤 Uploading files to MLflow server...\")\n",
    "        files_to_upload = [\n",
    "            (\"inputs array\", inputs_path),\n",
    "            (\"target array\", target_path),\n",
    "            (\"target names\", target_names_path),\n",
    "            (\"positive counts\", pos_counts_path)\n",
    "        ]\n",
    "        \n",
    "        for desc, filepath in tqdm(files_to_upload, desc=\"Uploading files\"):\n",
    "            print(f\"  - Uploading {desc}...\")\n",
    "            mlflow.log_artifact(filepath)\n",
    "            time.sleep(0.2)  # Small delay for progress visibility\n",
    "\n",
    "        print(\"📊 Logging metadata...\")\n",
    "        # Log parameters for browsing\n",
    "        mlflow.log_param(\"target_names\", target_names)\n",
    "        mlflow.log_param(\"input_features\", inputs.shape[1])\n",
    "        mlflow.log_param(\"output_features\", target.shape[1])\n",
    "        mlflow.log_param(\"num_samples\", n_samples)\n",
    "\n",
    "        pos_counts_dict = {name: int(count) for name, count in zip(target_names, pos_counts)}\n",
    "        mlflow.log_param(\"pos_counts\", pos_counts_dict)\n",
    "\n",
    "        # Log description\n",
    "        if description:\n",
    "            mlflow.set_tag(\"description\", description)\n",
    "            \n",
    "        print(f\"✅ Dataset upload complete! View at: {MLFLOW_TRACKING_URI}/#/experiments/{run.info.experiment_id}/runs/{run.info.run_id}\")\n",
    "        return run.info.run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fa7a412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FTIR dataset from all splits...\n",
      "Loading 6342 samples from train split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading train: 100%|███████████████████████████████████████████████| 6342/6342 [00:01<00:00, 3603.63sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 1387 samples from valid split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading valid: 100%|███████████████████████████████████████████████| 1387/1387 [00:00<00:00, 1956.19sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 933 samples from test split...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading test: 100%|██████████████████████████████████████████████████| 933/933 [00:00<00:00, 2705.27sample/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset statistics:\n",
      "Total samples: 8662\n",
      "Feature dimension: 3602\n",
      "Number of classes: 17\n",
      "Positive samples per class:\n",
      " - alkane: 5986 (69.11%)\n",
      " - methyl: 5557 (64.15%)\n",
      " - alkene: 1165 (13.45%)\n",
      " - alkyne: 227 (2.62%)\n",
      " - alcohols: 2339 (27.00%)\n",
      " - amines: 817 (9.43%)\n",
      " - nitriles: 375 (4.33%)\n",
      " - aromatics: 5018 (57.93%)\n",
      " - alkyl halides: 2405 (27.76%)\n",
      " - esters: 961 (11.09%)\n",
      " - ketones: 787 (9.09%)\n",
      " - aldehydes: 207 (2.39%)\n",
      " - carboxylic acids: 629 (7.26%)\n",
      " - ether: 2155 (24.88%)\n",
      " - acyl halides: 96 (1.11%)\n",
      " - amides: 165 (1.90%)\n",
      " - nitro: 443 (5.11%)\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "class_names = [\"alkane\", \"methyl\", \"alkene\", \"alkyne\", \"alcohols\", \"amines\", \"nitriles\", \"aromatics\",\n",
    " \"alkyl halides\", \"esters\", \"ketones\", \"aldehydes\", \"carboxylic acids\", \"ether\",\n",
    " \"acyl halides\", \"amides\", \"nitro\"]\n",
    "\n",
    "# Define the data directory\n",
    "data_dir = \"../data/ftir\"\n",
    "splits = [\"train\", \"valid\", \"test\"]\n",
    "\n",
    "# Initialize arrays to store combined data\n",
    "all_inputs = []\n",
    "all_targets = []\n",
    "\n",
    "# Load data from all splits\n",
    "print(\"Loading FTIR dataset from all splits...\")\n",
    "for split in splits:\n",
    "    split_dir = os.path.join(data_dir, split)\n",
    "    \n",
    "    # Get all sample IDs\n",
    "    npy_paths = glob(os.path.join(split_dir, \"*.npy\"))\n",
    "    ids = [int(os.path.splitext(os.path.basename(path))[0]) for path in npy_paths]\n",
    "    ids.sort()\n",
    "    \n",
    "    print(f\"Loading {len(ids)} samples from {split} split...\")\n",
    "    \n",
    "    # Load each sample\n",
    "    for sample_id in tqdm(ids, desc=f\"Loading {split}\", unit=\"sample\"):\n",
    "        npy_path = os.path.join(split_dir, f\"{sample_id}.npy\")\n",
    "        txt_path = os.path.join(split_dir, f\"{sample_id}.txt\")\n",
    "        \n",
    "        # Load feature vector\n",
    "        x = np.load(npy_path)\n",
    "        \n",
    "        # Load target labels\n",
    "        with open(txt_path, \"r\") as f:\n",
    "            y = np.array([int(tok) for tok in f.read().strip().split()], dtype=np.int32)\n",
    "        \n",
    "        all_inputs.append(x)\n",
    "        all_targets.append(y)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "inputs = np.vstack(all_inputs)  # Stack vertically to create (n_samples, n_features)\n",
    "target = np.vstack(all_targets)  # Stack vertically to create (n_samples, n_classes)\n",
    "\n",
    "# Print dataset statistics\n",
    "print(f\"\\nDataset statistics:\")\n",
    "print(f\"Total samples: {inputs.shape[0]}\")\n",
    "print(f\"Feature dimension: {inputs.shape[1]}\")\n",
    "print(f\"Number of classes: {len(class_names)}\")\n",
    "print(f\"Positive samples per class:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    count = np.sum(target[:, i])\n",
    "    percent = count / len(target) * 100\n",
    "    print(f\" - {name}: {count} ({percent:.2f}%)\")\n",
    "\n",
    "# Package the data in the required format\n",
    "data = {\n",
    "    \"inputs\": inputs,\n",
    "    \"target\": target,\n",
    "    \"target_names\": class_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4376590e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 Starting dataset upload process for 'ftir_complete' - combined_splits\n",
      "✅ Validating dataset format...\n",
      "📊 Dataset stats: 8662 samples, 3602 input features, 17 target classes\n",
      "🔍 Connecting to MLflow server...\n",
      "✅ Created run with ID: 6e1cfb26044743a59fdc7486be749680\n",
      "📝 Locating and saving notebook source...\n",
      "✅ Found notebook at: /Users/jekabsgritans/uni/cbl/mt_fcg/scripts/generate_ftir.ipynb\n",
      "✅ Notebook logged successfully\n",
      "💾 Saving dataset files locally...\n",
      "  - Saving inputs array ((8662, 3602))...\n",
      "  - Saving target array ((8662, 17))...\n",
      "  - Saving target names...\n",
      "  - Saving positive counts...\n",
      "📤 Uploading files to MLflow server...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:   0%|                                                                 | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Uploading inputs array...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  25%|██████████████▎                                          | 1/4 [00:03<00:11,  3.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Uploading target array...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  50%|████████████████████████████▌                            | 2/4 [00:04<00:03,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Uploading target names...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files:  75%|██████████████████████████████████████████▊              | 3/4 [00:05<00:01,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - Uploading positive counts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading files: 100%|█████████████████████████████████████████████████████████| 4/4 [00:05<00:00,  1.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Logging metadata...\n",
      "✅ Dataset upload complete! View at: https://mlflow.gritans.lv/#/experiments/6/runs/6e1cfb26044743a59fdc7486be749680\n",
      "🏃 View run combined_splits at: https://data_user:ais7Rah2foo0gee9@mlflow.gritans.lv/#/experiments/6/runs/6e1cfb26044743a59fdc7486be749680\n",
      "🧪 View experiment at: https://data_user:ais7Rah2foo0gee9@mlflow.gritans.lv/#/experiments/6\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'6e1cfb26044743a59fdc7486be749680'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload the FTIR dataset to MLflow\n",
    "upload_dataset(\n",
    "    data=data,\n",
    "    dataset_name=\"ftir_complete\",\n",
    "    version_name=\"combined_splits\",\n",
    "    description=\"\"\"\n",
    "    Complete FTIR spectroscopy dataset combined from train, validation, and test splits.\n",
    "    \n",
    "    This dataset contains FTIR (Fourier-transform infrared) spectroscopy data from the FCG-former paper,\n",
    "    with 17 functional group classes. Each sample is labeled with the presence/absence of each functional group.\n",
    "    \n",
    "    Features: FTIR spectra\n",
    "    Targets: Binary labels for 17 functional groups (alkane, methyl, alkene, etc.)\n",
    "    Combined from: train, validation, and test splits\n",
    "    \"\"\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
