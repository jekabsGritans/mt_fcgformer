{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1466c028",
   "metadata": {},
   "source": [
    "%%markdown\n",
    "# Dataset Generation Template\n",
    "\n",
    "This notebook provides a template for generating and uploading new datasets to MLflow, after you have the raw scraped data.\n",
    "\n",
    "## Data Format\n",
    "\n",
    "Your data needs to be organized in a dictionary with three required keys:\n",
    "- `inputs`: Feature matrix as NumPy array\n",
    "- `target`: Binary target matrix as NumPy array\n",
    "- `target_names`: List of strings naming each target feature\n",
    "\n",
    "### Shape Requirements\n",
    "\n",
    "| Component | Type | Shape | Description | Example |\n",
    "|-----------|------|-------|-------------|----------|\n",
    "| `inputs` | `np.ndarray` | `(n_samples, n_features)` | Each row is one sample, each column a feature | `(1000, 10)` for 1000 samples with 10 features |\n",
    "| `target` | `np.ndarray` | `(n_samples, n_targets)` | Binary matrix where each column represents one target | `(1000, 3)` for 1000 samples with 3 possible targets |\n",
    "| `target_names` | `list[str]` | `(n_targets,)` | Names for each target column | `[\"cat\", \"dog\", \"bird\"]` for 3 targets |\n",
    "\n",
    "## Example\n",
    "\n",
    "```python\n",
    "data = {\n",
    "    \"inputs\": np.array([\n",
    "        [0.1, 0.2, 0.3],  # Sample 1 with 3 features\n",
    "        [0.4, 0.5, 0.6],  # Sample 2 with 3 features\n",
    "        # ... more samples\n",
    "    ]),\n",
    "    \"target\": np.array([\n",
    "        [1, 0],  # Sample 1: positive for target 1, negative for target 2\n",
    "        [0, 1],  # Sample 2: negative for target 1, positive for target 2\n",
    "        # ... more samples\n",
    "    ]),\n",
    "    \"target_names\": [\"group_A\", \"group_B\"]  # Names for the two target columns\n",
    "}\n",
    "```\n",
    "\n",
    "## Uploading the dataset\n",
    "Once your data is prepared, use `upload_dataset()` to save it to MLflow.\n",
    "This will verify that the data is formatted correctly and then upload it to the server.\n",
    "\n",
    "```python\n",
    "upload_dataset(\n",
    "    data=data,\n",
    "    dataset_name=\"ftir_no_bonding_effects\",  # Broader dataest for which we can have multiple versions\n",
    "    version_name=\"initial_data\",  # Description of this version\n",
    "    description=\"FTIR dataset downloaded from the FCGFormer paper without any modifications\"  # Optional details\n",
    ")\n",
    "```\n",
    "\n",
    "## Accessing the Dataset\n",
    "\n",
    "After upload, the dataset will be available in MLflow for model training with:\n",
    "- NumPy arrays saved as `.npy` files\n",
    "- Target names and counts (number of positive examples) in text files\n",
    "- The code of this notebook saved for reproducability (so you don't have to upload it anywhere)\n",
    "\n",
    "You can view your dataset in MLflow by opening the link printed after calling `upload_dataset()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d458a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# This cell defines upload_dataset. You can ignore it.\n",
    "\n",
    "# Install required packages for `upload_dataset()` with quiet flag to reduce output\n",
    "%pip install -q numpy mlflow ipynbname requests tqdm scikit-learn scikit-multilearn\n",
    "\n",
    "import os\n",
    "import urllib.parse\n",
    "import jupyter_client\n",
    "from typing import Dict, Any\n",
    "import time\n",
    "from tqdm.notebook import tqdm\n",
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "try:\n",
    "    import mlflow\n",
    "except ImportError:\n",
    "    %pip install -q mlflow\n",
    "    import mlflow\n",
    "\n",
    "try:\n",
    "    import requests\n",
    "except ImportError:\n",
    "    %pip install -q requests\n",
    "    import requests\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "except ImportError:\n",
    "    %pip install -q numpy\n",
    "    import numpy as np\n",
    "\n",
    "try:\n",
    "    from notebook import notebookapp\n",
    "except ImportError:\n",
    "    from jupyter_server import serverapp as notebookapp\n",
    "\n",
    "# MLFlow creds\n",
    "MLFLOW_DOMAIN = \"mlflow.gritans.lv\"\n",
    "MLFLOW_USERNAME = \"data_user\"\n",
    "MLFLOW_PASSWORD = \"ais7Rah2foo0gee9\"\n",
    "MLFLOW_TRACKING_URI = f\"https://{MLFLOW_DOMAIN}\"\n",
    "\n",
    "parsed_uri = urllib.parse.urlparse(MLFLOW_TRACKING_URI)\n",
    "auth_uri = parsed_uri._replace(\n",
    "    netloc=f\"{urllib.parse.quote(MLFLOW_USERNAME)}:{urllib.parse.quote(MLFLOW_PASSWORD)}@{parsed_uri.netloc}\"\n",
    ").geturl()\n",
    "\n",
    "mlflow.set_tracking_uri(auth_uri)\n",
    "\n",
    "def upload_dataset(\n",
    "    data: Dict[str, Any],\n",
    "    dataset_name: str,\n",
    "    version_name: str,\n",
    "    description: str | None = None,\n",
    "    split: bool = True,\n",
    "    test_size: float = 0.2,\n",
    "    valid_size: float = 0.2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (Dict[str, Any]): Dictionary containing the dataset with keys:\n",
    "            - \"inputs\": NumPy array of shape (num_samples, num_input_features)\n",
    "            - \"target\": NumPy array of shape (num_samples, num_target_features)\n",
    "            - \"target_names\": List of target feature names, in the same order as the target array.\n",
    "        dataset_name (str): Name of the dataset.\n",
    "        version_name (str): A descriptive version name for the dataset. \n",
    "        description (str): An (optional) description of this dataset version.\n",
    "        split (bool): Whether to create train/valid/test splits.\n",
    "        test_size (float): Fraction of data for test set.\n",
    "        valid_size (float): Fraction of remaining data for validation.\n",
    "    \"\"\"\n",
    "    print(f\"🚀 Starting dataset upload process for '{dataset_name}' - {version_name}\")\n",
    "    \n",
    "    # Check dictionary\n",
    "    print(\"✅ Validating dataset format...\")\n",
    "    expected_keys = {\"inputs\", \"target\", \"target_names\"}\n",
    "    assert set(data.keys()) == expected_keys, (\n",
    "        f\"Invalid dataset format. Keys should be {expected_keys}.\"\n",
    "    )\n",
    "\n",
    "    # Check expected types\n",
    "    assert isinstance(data[\"inputs\"], np.ndarray), (\n",
    "        f\"Inputs must be a numpy.ndarray. Got {type(data['inputs'])}.\"\n",
    "    )\n",
    "    assert isinstance(data[\"target\"], np.ndarray), (\n",
    "        f\"Targets must be a numpy.ndarray. Got {type(data['target'])}.\"\n",
    "    )\n",
    "    assert isinstance(data[\"target_names\"], list), (\n",
    "        f\"target names must be a list. Got {type(data['target_names'])}.\"\n",
    "    )\n",
    "    assert all(isinstance(name, str) for name in data[\"target_names\"]), (\n",
    "        \"All target names must be strings.\"\n",
    "    )\n",
    "\n",
    "    # Check expected shapes\n",
    "    inputs: np.ndarray = data[\"inputs\"]\n",
    "    target: np.ndarray = data[\"target\"]\n",
    "    target_names = data[\"target_names\"]\n",
    "\n",
    "    assert inputs.ndim == 2, (\n",
    "        f\"Inputs must be a (num_samples, num_input_features) array. \"\n",
    "        f\"Got {inputs.ndim} dimensions.\"\n",
    "    )\n",
    "    assert target.ndim == 2, (\n",
    "        f\"Targets must be a (num_samples, num_target_features) array. \"\n",
    "        f\"Got {target.ndim} dimensions.\"\n",
    "    )\n",
    "\n",
    "    n_samples = inputs.shape[0]\n",
    "    assert n_samples > 0, (\n",
    "        f\"Inputs must have at least one sample. Got {n_samples} samples.\"\n",
    "    )\n",
    "    assert n_samples == target.shape[0], (\n",
    "        f\"Inputs and targets must have the same number of samples. \"\n",
    "        f\"Got {n_samples} inputs and {target.shape[0]} targets.\"\n",
    "    )\n",
    "\n",
    "    n_target_features = target.shape[1]\n",
    "    assert n_target_features > 0 and n_target_features == len(target_names), (\n",
    "        f\"Targets must have the same number of features as target names. \"\n",
    "        f\"Got {n_target_features} target features and {len(target_names)} target names.\"\n",
    "    )\n",
    "\n",
    "    # Create train/valid/test splits if requested\n",
    "    splits = {}\n",
    "    \n",
    "    if split:\n",
    "        print(\"🔪 Creating train/valid/test splits...\")\n",
    "        \n",
    "        # For multi-label data, we need to use iterative stratification\n",
    "        print(\"  - Using iterative stratification for multi-label data\")\n",
    "        \n",
    "        # First split off the test set\n",
    "        train_valid_inputs, train_valid_target, test_inputs, test_target = iterative_train_test_split(\n",
    "            inputs, target, test_size=test_size\n",
    "        )\n",
    "        \n",
    "        # Split remaining data into train/valid\n",
    "        train_inputs, train_target, valid_inputs, valid_target = iterative_train_test_split(\n",
    "            train_valid_inputs, train_valid_target, test_size=valid_size\n",
    "        )\n",
    "        \n",
    "        # Store the splits\n",
    "        splits = {\n",
    "            \"train\": (train_inputs, train_target),\n",
    "            \"valid\": (valid_inputs, valid_target),\n",
    "            \"test\": (test_inputs, test_target)\n",
    "        }\n",
    "        \n",
    "        # Print split statistics\n",
    "        print(\"📊 Split statistics:\")\n",
    "        for split_name, (split_x, split_y) in splits.items():\n",
    "            split_size = len(split_x)\n",
    "            split_pct = split_size / n_samples * 100\n",
    "            print(f\"  - {split_name}: {split_size} samples ({split_pct:.1f}%)\")\n",
    "    else:\n",
    "        # Just use the original dataset\n",
    "        splits = {\"full\": (inputs, target)}\n",
    "    \n",
    "    # Connect to MLFlow\n",
    "    print(\"🔍 Connecting to MLflow server...\")\n",
    "    mlflow.set_experiment(experiment_name=dataset_name)\n",
    "    with mlflow.start_run(run_name=version_name, description=description) as run:\n",
    "        print(f\"✅ Created run with ID: {run.info.run_id}\")\n",
    "        local_dir = os.path.join(\"./runs\", run.info.run_id)\n",
    "        os.makedirs(local_dir, exist_ok=True)\n",
    "\n",
    "        # Log the notebook generating this dataset\n",
    "        print(\"📝 Locating and saving notebook source...\")\n",
    "        try:\n",
    "            # primary: ipynbname often just works\n",
    "            import ipynbname\n",
    "            notebook_path = ipynbname.path()\n",
    "            print(f\"✅ Found notebook at: {notebook_path}\")\n",
    "        except Exception:\n",
    "            # Fallback notebook location code - unchanged\n",
    "            print(\"⚠️ Primary notebook path detection failed, trying alternative method...\")\n",
    "            # ... existing fallback code ...\n",
    "            conn_file = jupyter_client.find_connection_file()\n",
    "            kernel_id = os.path.basename(conn_file).split('-', 1)[1].split('.')[0]\n",
    "\n",
    "            # 2) iterate over all running notebook servers\n",
    "            for srv in notebookapp.list_running_servers():\n",
    "                # build the URL for sessions\n",
    "                url = srv['url'].rstrip('/') + '/api/sessions'\n",
    "                token = srv.get('token', '')\n",
    "                params = {'token': token} if token else {}\n",
    "\n",
    "                try:\n",
    "                    resp = requests.get(url, params=params)\n",
    "                    resp.raise_for_status()\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "                # 3) look for our kernel in their active sessions\n",
    "                for sess in resp.json():\n",
    "                    if sess['kernel']['id'] == kernel_id:\n",
    "                        # 4) reconstruct the full path\n",
    "                        rel_path = sess['notebook']['path']       # e.g. \"subdir/MyNotebook.ipynb\"\n",
    "                        notebook_path = os.path.join(srv['notebook_dir'], rel_path)\n",
    "                        print(f\"✅ Found notebook at: {notebook_path}\")\n",
    "                        break\n",
    "                else:\n",
    "                    continue\n",
    "                break\n",
    "            else:\n",
    "                raise RuntimeError(\"Could not locate the current notebook path\")\n",
    "\n",
    "        mlflow.log_artifact(notebook_path)\n",
    "        \n",
    "        # Log dataset files - one set per split\n",
    "        print(\"📄 Preparing and uploading dataset files...\")\n",
    "        \n",
    "        target_names_path = os.path.join(local_dir, \"target_names.txt\")\n",
    "        with open(target_names_path, \"w\") as f:\n",
    "            for name in target_names:\n",
    "                f.write(f\"{name}\\n\")\n",
    "        \n",
    "        files_to_upload = [(\"target names\", target_names_path)]\n",
    "        \n",
    "        # Save each split with its own files\n",
    "        for split_name, (split_inputs, split_target) in splits.items():\n",
    "            print(f\"  - Processing {split_name} split...\")\n",
    "            \n",
    "            # Calculate positive counts for this split\n",
    "            split_pos_counts = split_target.sum(axis=0)\n",
    "            \n",
    "            # Save split-specific files\n",
    "            split_inputs_path = os.path.join(local_dir, f\"{split_name}_inputs.npy\")\n",
    "            split_target_path = os.path.join(local_dir, f\"{split_name}_target.npy\")\n",
    "            split_pos_counts_path = os.path.join(local_dir, f\"{split_name}_pos_counts.txt\")\n",
    "            \n",
    "            # Save numpy arrays\n",
    "            np.save(split_inputs_path, split_inputs)\n",
    "            np.save(split_target_path, split_target)\n",
    "            \n",
    "            # Save positive counts\n",
    "            with open(split_pos_counts_path, \"w\") as f:\n",
    "                for i, count in enumerate(split_pos_counts):\n",
    "                    f.write(f\"{count}\\n\")\n",
    "            \n",
    "            # Add to upload list\n",
    "            files_to_upload.extend([\n",
    "                (f\"{split_name} inputs\", split_inputs_path),\n",
    "                (f\"{split_name} target\", split_target_path),\n",
    "                (f\"{split_name} pos counts\", split_pos_counts_path)\n",
    "            ])\n",
    "\n",
    "            pos_counts_dict = {name: int(count) for name, count in zip(target_names, split_pos_counts)}\n",
    "            mlflow.log_param(f\"{split_name}_positive_samples\", pos_counts_dict)\n",
    "            mlflow.log_param(f\"{split_name}_num_samples\", len(split_inputs))\n",
    "\n",
    "        \n",
    "        # Upload all files to MLFlow\n",
    "        print(\"📤 Uploading files to MLflow server...\")\n",
    "        for desc, filepath in tqdm(files_to_upload, desc=\"Uploading files\"):\n",
    "            print(f\"  - Uploading {desc}...\")\n",
    "            mlflow.log_artifact(filepath)\n",
    "            time.sleep(0.2)  # Small delay for progress visibility\n",
    "\n",
    "        # Log metadata\n",
    "        print(\"📊 Logging metadata...\")\n",
    "        mlflow.log_param(\"target_names\", target_names)\n",
    "        mlflow.log_param(\"input_features\", inputs.shape[1])\n",
    "        mlflow.log_param(\"target_features\", target.shape[1])\n",
    "        mlflow.log_param(\"total_num_samples\", n_samples)\n",
    "        \n",
    "        if description:\n",
    "            mlflow.set_tag(\"description\", description)\n",
    "        \n",
    "        print(f\"✅ Dataset upload complete! View at: {MLFLOW_TRACKING_URI}/#/experiments/{run.info.experiment_id}/runs/{run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa7a412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "class_names = [\"alkane\", \"methyl\", \"alkene\", \"alkyne\", \"alcohols\", \"amines\", \"nitriles\", \"aromatics\",\n",
    " \"alkyl halides\", \"esters\", \"ketones\", \"aldehydes\", \"carboxylic acids\", \"ether\",\n",
    " \"acyl halides\", \"amides\", \"nitro\"]\n",
    "\n",
    "# Define the data directory\n",
    "data_dir = \"../data/ftir\"\n",
    "\n",
    "# Initialize arrays to store data\n",
    "all_inputs = []\n",
    "all_targets = []\n",
    "\n",
    "# We'll combine all the data from existing splits\n",
    "print(\"Loading FTIR dataset...\")\n",
    "\n",
    "# Get all sample IDs across all directories\n",
    "all_npy_paths = glob(os.path.join(data_dir, \"**/*.npy\"), recursive=True)\n",
    "all_ids_with_paths = [(int(os.path.splitext(os.path.basename(path))[0]), os.path.dirname(path)) \n",
    "                      for path in all_npy_paths]\n",
    "all_ids_with_paths.sort()  # Sort by ID\n",
    "\n",
    "print(f\"Found {len(all_ids_with_paths)} total samples\")\n",
    "\n",
    "# Load each sample\n",
    "for sample_id, dir_path in tqdm(all_ids_with_paths, desc=\"Loading samples\", unit=\"sample\"):\n",
    "    npy_path = os.path.join(dir_path, f\"{sample_id}.npy\")\n",
    "    txt_path = os.path.join(dir_path, f\"{sample_id}.txt\")\n",
    "    \n",
    "    # Load feature vector\n",
    "    x = np.load(npy_path)\n",
    "    \n",
    "    # Load target labels\n",
    "    with open(txt_path, \"r\") as f:\n",
    "        y = np.array([int(tok) for tok in f.read().strip().split()], dtype=np.int32)\n",
    "    \n",
    "    all_inputs.append(x)\n",
    "    all_targets.append(y)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "inputs = np.vstack(all_inputs)  # Stack vertically to create (n_samples, n_features)\n",
    "target = np.vstack(all_targets)  # Stack vertically to create (n_samples, n_classes)\n",
    "\n",
    "print(f\"Dataset loaded: {inputs.shape[0]} samples, {inputs.shape[1]} features, {target.shape[1]} classes\")\n",
    "\n",
    "# Package the data in the required format\n",
    "data = {\n",
    "    \"inputs\": inputs,\n",
    "    \"target\": target,\n",
    "    \"target_names\": class_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4376590e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload the FTIR dataset to MLflow\n",
    "upload_dataset(\n",
    "    data=data,\n",
    "    dataset_name=\"ftir_fcg\",\n",
    "    version_name=\"custom_split\",\n",
    "    description=\"FTIR spectroscopy dataset with 17 functional group targets from thr FCGFormer paper\",\n",
    "    split=True  # Let upload_dataset handle the splitting\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
